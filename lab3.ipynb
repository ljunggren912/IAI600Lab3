{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is based on Agerons code at github https://github.com/ageron/handson-ml3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"end_to_end_project\" \n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables for path for download, path for saving file and function for fetching the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ljung\\AppData\\Local\\Temp\\ipykernel_5608\\1387014147.py:15: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  housing_tgz.extractall(path=housing_path)\n"
     ]
    }
   ],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a \"load to pandas DataFrame\" function and call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Splitting the dataset into two datasets, features and labels\n",
    "housing_features = housing.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = housing[\"median_house_value\"]\n",
    "\n",
    "#Create training and test sets (random)\n",
    "housing_train_features, housing_test_features, housing_train_labels, housing_test_labels = train_test_split(housing_features, housing_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, n_init=10,\n",
    "                              random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n",
    "    \n",
    "\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
    "        X = check_array(X)  # checks that X is an array with finite float values\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.scale_ = X.std(axis=0)\n",
    "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines for data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.compose import make_column_selector, ColumnTransformer\n",
    "\n",
    "#Pipline to handle categories, it fills in (impute) null values with the most frequent value and then encodes it with OneHot encoding\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"impute\",SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\",OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"]  # feature names out\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler())\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler())\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                                     StandardScaler())\n",
    "\n",
    "#Pipline to handle categories, it fills in (impute) null values with the most frequent value and then encodes it with OneHot encoding\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"impute\",SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\",OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer([\n",
    "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "                               \"households\", \"median_income\"]),\n",
    "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline)  # one column remaining: housing_median_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines for training the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#Creates a linear regression pipeline with preprocessing\n",
    "lin_reg_pipeline = Pipeline([\n",
    "    (\"preprocessing\",preprocessing_pipeline), \n",
    "    (\"linear_regression\", LinearRegression())\n",
    "])\n",
    "\n",
    "#Creates a decision tree pipeline with preprocessing\n",
    "decision_tree_reg_pipeline = Pipeline([\n",
    "    (\"preprocessing\",preprocessing_pipeline),\n",
    "    (\"decision_tree\", DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "random_forest_reg_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_pipeline),\n",
    "    (\"random_forest_regression\", RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "#Creates a SVM(regression) pipeline\n",
    "\n",
    "svr_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_pipeline),\n",
    "    (\"SVR\", SVR())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define models to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Linear Regression\": lin_reg_pipeline,\n",
    "    \"Decision Tree\" : decision_tree_reg_pipeline,\n",
    "    \"Random Forest\": random_forest_reg_pipeline,\n",
    "    \"SVR\": svr_pipeline\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a parametergrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_search = {\n",
    "    \"Linear Regression\": {\n",
    "        'linear_regression__fit_intercept':[True,False],\n",
    "        'linear_regression__positive':[True,False]\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        'decision_tree__max_depth': [5, 10, 15, None],\n",
    "        'decision_tree__min_samples_split': [2, 10, 20]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'random_forest_regression__n_estimators': [50, 100, 200],\n",
    "        'random_forest_regression__max_depth': [10, 20, 30, None],\n",
    "        'random_forest_regression__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    \"SVR\": {\n",
    "        'SVR__C': [0.1, 1, 10],\n",
    "        'SVR__epsilon': [0.01, 0.1, 0.2],\n",
    "        'SVR__kernel': ['linear', 'rbf']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch for the models defined and store the results in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty dictionary to store reports for each model\n",
    "models_report_grid_search = {}\n",
    "\n",
    "# Timing and applying GridSearchCV\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()  # Start the timer\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, \n",
    "        param_grid=param_grid_search[name], \n",
    "        cv=5, \n",
    "        scoring=\"neg_root_mean_squared_error\", \n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(housing_train_features, housing_train_labels)\n",
    "    \n",
    "    end_time = time.time()  # End the timer\n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    \n",
    "    # Build a report for the current model\n",
    "    report = {\n",
    "        \"Model\": name,\n",
    "        \"Best Hyperparameters\": grid_search.best_params_,\n",
    "        \"Best Score (RMSE)\": -grid_search.best_score_,  # negate to get positive RMSE\n",
    "        \"Time Taken (s)\": elapsed_time,  # Time taken for the grid search\n",
    "        \"Number of samples in training set\": housing_train_features.shape[0]\n",
    "    }\n",
    "\n",
    "    # Create and sort the testing parameters DataFrame\n",
    "    testing_params = pd.DataFrame(grid_search.cv_results_)\n",
    "    testing_params = testing_params.sort_values(by='rank_test_score')\n",
    "    cols = ['rank_test_score'] + [col for col in testing_params.columns if col != 'rank_test_score']\n",
    "    testing_params = testing_params[cols]\n",
    "\n",
    "    # Add the sorted testing parameters to the report\n",
    "    report['Testing Parameters'] = testing_params\n",
    "    \n",
    "    # Append the report to the dictionary\n",
    "    models_report_grid_search[name] = report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a excel file with the results and stores it in the project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary into a DataFrame for the main overview report, excluding \"Testing Parameters\"\n",
    "df_grid_search_report = pd.DataFrame([{k: v for k, v in report.items() if k != 'Testing Parameters'} \n",
    "                                      for report in models_report_grid_search.values()])\n",
    "\n",
    "# Write to Excel\n",
    "with pd.ExcelWriter('model_search_results.xlsx') as writer:\n",
    "    # Write the main overview sheet without the \"Testing Parameters\"\n",
    "    df_grid_search_report.to_excel(writer, sheet_name='Grid Search Results', index=False)\n",
    "    \n",
    "    # Write the detailed testing parameters for each model to separate sheets\n",
    "    for model_name, report in models_report_grid_search.items():\n",
    "        if \"Testing Parameters\" in report:\n",
    "            report[\"Testing Parameters\"].to_excel(writer, sheet_name=f'{model_name}_Parameters', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_grid_random = {\n",
    "    \"Linear Regression\": {\n",
    "        'linear_regression__fit_intercept': [True, False],\n",
    "        'linear_regression__positive': [True, False]\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        'decision_tree__max_depth': randint(5, 16),  # Randomly sample between 5 and 15, inclusive\n",
    "        'decision_tree__min_samples_split': randint(2, 21)  # Randomly sample between 2 and 20\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'random_forest_regression__n_estimators': randint(50, 201),  # Randomly sample between 50 and 200\n",
    "        'random_forest_regression__max_depth': randint(10, 31),  # Randomly sample between 10 and 30\n",
    "        'random_forest_regression__min_samples_split': randint(2, 11)  # Randomly sample between 2 and 10\n",
    "    },\n",
    "    \"SVR\": {\n",
    "        'SVR__C': uniform(0.1, 9.9),  # Sample from a continuous uniform distribution between 0.1 and 9.9\n",
    "        'SVR__epsilon': uniform(0.01, 0.19),  # Sample from a continuous uniform distribution between 0.01 and 0.19\n",
    "        'SVR__kernel': ['linear', 'rbf']  # For kernel, we still use a list since it's categorical\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty dictionary to store reports for each model\n",
    "models_report_random_search = {}\n",
    "\n",
    "# Timing and applying RandomizedSearchCV\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()  # Start the timer\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model, \n",
    "        param_distributions=param_grid_random[name], \n",
    "        cv=5, \n",
    "        n_iter=10, \n",
    "        scoring=\"neg_root_mean_squared_error\", \n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    random_search.fit(housing_train_features, housing_train_labels)\n",
    "    \n",
    "    end_time = time.time()  # End the timer\n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    \n",
    "    # Build a report for the current model\n",
    "    report = {\n",
    "        \"Model\": name,\n",
    "        \"Best Hyperparameters\": random_search.best_params_,\n",
    "        \"Best Score (RMSE)\": -random_search.best_score_,  # negate to get positive RMSE\n",
    "        \"Time Taken (s)\": elapsed_time,  # Time taken for the random search\n",
    "        \"Number of samples in training set\": housing_train_features.shape[0]\n",
    "    }\n",
    "\n",
    "    # Create and sort the testing parameters DataFrame\n",
    "    testing_params = pd.DataFrame(random_search.cv_results_)\n",
    "    testing_params = testing_params.sort_values(by='rank_test_score')\n",
    "    cols = ['rank_test_score'] + [col for col in testing_params.columns if col != 'rank_test_score']\n",
    "    testing_params = testing_params[cols]\n",
    "\n",
    "    # Add the sorted testing parameters to the report\n",
    "    report['Testing Parameters'] = testing_params\n",
    "    \n",
    "    # Append the report to the dictionary\n",
    "    models_report_random_search[name] = report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stores the results in a excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the dictionary into a DataFrame for the main overview report\n",
    "df_random_search_report = pd.DataFrame([{k: v for k, v in report.items() if k != 'Testing Parameters'} \n",
    "                                         for report in models_report_random_search.values()])\n",
    "\n",
    "# Write to Excel\n",
    "with pd.ExcelWriter('random_search_results.xlsx') as writer:\n",
    "    # Write the main overview sheet without the \"Testing Parameters\"\n",
    "    df_random_search_report.to_excel(writer, sheet_name='Random Search Results', index=False)\n",
    "    \n",
    "    # Write the detailed testing parameters for each model to separate sheets\n",
    "    for model_name, report in models_report_random_search.items():\n",
    "        if \"Testing Parameters\" in report:\n",
    "            report[\"Testing Parameters\"].to_excel(writer, sheet_name=f'{model_name}_Parameters', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best model with tuned parameters on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "random_forest_reg_pipeline_best = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_pipeline),\n",
    "    (\"random_forest_regression\", RandomForestRegressor(max_depth=30, min_samples_split=2,n_estimators=200,random_state=42, n_jobs=-1))\n",
    "])\n",
    "random_forest_reg_pipeline_best.fit(housing_train_features, housing_train_labels)\n",
    "\n",
    "predictions = random_forest_reg_pipeline_best.predict(housing_test_features)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(housing_test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47680.806229532645\n"
     ]
    }
   ],
   "source": [
    "print(rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
